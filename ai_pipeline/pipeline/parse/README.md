

# LLM-based Resume Parsing Pipeline

This module processes text chunks (generated by the `extract` module) and uses a Large Language Model (LLM) to parse them into a structured JSON format, such as a resume.

## Features

-   **LLM-Powered Parsing**: Utilizes configurable LLM providers (e.g., OpenAI, local models, cloud endpoints) to understand and structure unstructured text.
-   **Provider Fallback**: Automatically falls back to secondary providers if the primary one fails, ensuring robustness.
-   **Schema-Driven Output**: Validates and structures the output against a predefined JSON schema (`schema.json`).
-   **Multi-Step Pipelines**: Supports configurable pipelines with multiple steps (e.g., an initial parsing step followed by a validation step).
-   **Metadata Tracking**: Automatically generates a detailed metadata file, tracking the provider used, pipeline steps, processing time, and confidence scores.
-   **Secure Configuration**: Sensitive configuration data (like API keys) is redacted in the metadata for security.
-   **Modular Structure**: Designed with a modular architecture for easier maintenance, testing, and development.

## Modular Structure

The module is divided into several components to separate concerns:

-   `config.py`: Handles loading and merging of `config.json` and `global_providers.json`.
-   `data_processor.py`: Contains the core logic for parsing LLM responses, normalizing data, and performing final validation.
-   `parser.py`: The main orchestrator that provides the `parse_resume_data` function.
-   `file_utils.py`: Contains utility functions for file operations (shared with the `extract` module).
-   `main.py`: The command-line interface (CLI) for running the pipeline.
-   `schema.json`: Defines the required structure for the final parsed output.
-   `prompts/`: A directory containing system and user prompt templates for different LLM providers and pipeline steps.

## Prerequisites

-   Python 3.8+
-   The `json-repair` library. Install it via pip:
    ```bash
    pip install json-repair
    ```
-   Valid credentials or access for at least one configured LLM provider (e.g., `OPENAI_API_KEY` in your environment).
-   The `schema.json` file must be present in the `ai_pipeline/pipeline/parse/` directory.

## Usage

This pipeline is run via the command-line (CLI). **It is crucial to always run it as a module from the project root directory** to avoid `ModuleNotFoundError`.

### Basic Command

Navigate to your project's root directory in the terminal and run:

```bash
python -m ai_pipeline.pipeline.parse.main --input PATH/TO/CHUNKS_FILE.json
```

### CLI Arguments

| Argument | Short | Default | Description |
| :--- | :--- | :--- | :--- |
| `--input` | `-i` | (Required) | Path to the input JSON file containing text chunks. |
| `--output` | `-o` | (Auto-generated) | Path to the output parsed resume JSON file. |
| `--config` | `-c` | `ai_pipeline/pipeline/parse/parse_config.json` | Path to the parsing pipeline configuration file. |
| `--global-providers` | `-g` | `ai_pipeline/pipeline/config/global_providers.json` | Path to the global LLM providers configuration file. |

### Usage Example

```bash
python -m ai_pipeline.pipeline.parse.main \
  --input data/output/extract/chunks_resume_danuar.json \
  --output data/output/parse/resume_danuar_parsed.json
```

## Input & Output

### Input

-   **Chunks File (JSON)**: A JSON file containing an array of text strings, typically generated by the `extract` module.

### Output

The pipeline generates two files for each run:

1.  **Parsed Resume (JSON)**:
    -   Contains the structured data extracted from the text, conforming to `schema.json`.
    -   Includes `confidence` scores for different sections (e.g., `summary`, `work_experience`).
    -   Saved in the specified output directory or by default in `ai_pipeline/data/output/parse/`.

2.  **Metadata File (JSON)**:
    -   Contains audit information about the parsing process, such as:
        -   Source chunks file and timestamp.
        -   Status (`success` or `failed`).
        -   The LLM provider that was actually used.
        -   The pipeline configuration and steps executed.
        -   Processing time and confidence scores for each section.
        -   **Sensitive data (API keys, URLs) in provider config is redacted for security.**
    -   Saved in the `ai_pipeline/data/metadata/parse/` directory.
    -   Filename follows the format: `metadata_[original_filename].json`.

### Example Execution

After running the example command above, you will see output in your terminal:

```
--- Determining active provider by running first step...
--- Running Step: parse ---
--- Processing Chunk 1/2 ---
--- Active provider determined as: 'openai'. ---
--- Performing local structural cleaning on parsed data... ---
--- Starting Final Validation and Structural Cleaning ---

--- Result saved to 'data/output/parse/resume_danuar_parsed.json' ---
--- Metadata saved to 'data/metadata/parse/metadata_chunks_resume_danuar.json' ---
```

And two files will be created:
-   `data/output/parse/resume_danuar_parsed.json`
-   `data/metadata/parse/metadata_chunks_resume_danuar.json`